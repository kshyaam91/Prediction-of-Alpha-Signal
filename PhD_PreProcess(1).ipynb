{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UF8fAP3n1mVE"
   },
   "source": [
    "#Problem Statement: \n",
    "Predicting alpha signal using microblogging data\n",
    "\n",
    "#Problem Description:\n",
    "Alpha signal is used to make purchase decisions on stocks. This is done by calculating seven stock factors currently. To more accurately predict the Alpha signal, data is gathered from a microblogging site: 'StockTwits'. Brokers and stockers and tweet about the company in this microblogging site and tag the company with 'tickers', that start with a $ sign.\n",
    "\n",
    "Our goal is to do sentiment Analysis on the tweet data gathered from the microbloggin website and create a new stock factor, and inturn use this stock factor to predict the value of Alpha. Therfore, resulting in a better Aplha signal calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JR9ImBP75qOp"
   },
   "source": [
    "#Importing Required python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLfHi-0GSGlD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pandas_profiling as pp\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.probability import FreqDist\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import unidecode\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tcbf21nu55yu"
   },
   "source": [
    "#Reading the JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWWAfM4hSGlK"
   },
   "outputs": [],
   "source": [
    "with open ('PhD/train_data.json') as f:\n",
    "    json_data = json.load(f)\n",
    "    json_data = json_data['records']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aktW-BK06B3Q"
   },
   "source": [
    "#Converting the JSON Data into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "hNx8YDbNSGlN",
    "outputId": "696ef672-415a-4b30-9c8d-af2903571433"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>stocktwit_tweet</th>\n",
       "      <th>ticker</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>$AMD going up but hesitating however chart is ...</td>\n",
       "      <td>$AMD</td>\n",
       "      <td>2018-09-19 18:38:28+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>@inforlong @MariaGascon Despite\\nChina trade w...</td>\n",
       "      <td>$CAT</td>\n",
       "      <td>2018-10-09 03:51:06+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>$AVGO WTF?</td>\n",
       "      <td>$AVGO</td>\n",
       "      <td>2018-07-12 13:35:32+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>$PH\\n New Insider Filing On: \\n MULLER KLAUS P...</td>\n",
       "      <td>$PH</td>\n",
       "      <td>2018-07-19 03:32:50+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>$FB if it bounces tommorrow do the right thing...</td>\n",
       "      <td>$FB</td>\n",
       "      <td>2018-08-23 19:07:54+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>$FB as usual......RED  \\nevery day.</td>\n",
       "      <td>$FB</td>\n",
       "      <td>2018-08-02 11:40:49+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>$AET UnitedHealth Group Goes Ex-Dividend Today...</td>\n",
       "      <td>$AET</td>\n",
       "      <td>2018-09-07 15:01:06+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>$NOC let&amp;#39;s see when we can hit 310</td>\n",
       "      <td>$NOC</td>\n",
       "      <td>2018-09-12 20:48:40+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>@InCyD3R @Ferrabi I bet you‚Äôre also the one sa...</td>\n",
       "      <td>$AMZN</td>\n",
       "      <td>2018-10-28 01:19:27+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>$AMD Pastor the Stooge</td>\n",
       "      <td>$AMD</td>\n",
       "      <td>2018-08-17 13:35:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>$NFLX called it Friday hahahaha suckers sold F...</td>\n",
       "      <td>$NFLX</td>\n",
       "      <td>2018-08-27 12:51:21+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>$gww EXP:7/20/2018|MaxPain:292.5|HighPutOI:250...</td>\n",
       "      <td>$gww</td>\n",
       "      <td>2018-07-17 14:05:07+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>$NFLX Day in, Day out this puppy is making peo...</td>\n",
       "      <td>$NFLX</td>\n",
       "      <td>2018-10-18 15:36:57+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>$AAPL</td>\n",
       "      <td>$AAPL</td>\n",
       "      <td>2018-10-06 21:38:04+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>$ANTM</td>\n",
       "      <td>$ANTM</td>\n",
       "      <td>2018-08-08 13:34:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>$NWL Earnings August 06 BMO. 17-Aug-18 Straddl...</td>\n",
       "      <td>$NWL</td>\n",
       "      <td>2018-08-03 13:50:59+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>$AMD buying more</td>\n",
       "      <td>$AMD</td>\n",
       "      <td>2018-09-18 15:45:22+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>$mu mms took out the SLs, yet it still won&amp;#39...</td>\n",
       "      <td>$mu</td>\n",
       "      <td>2018-09-05 14:23:16+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>$MU it‚Äôs not even a downtrend - it‚Äôs a flush. ...</td>\n",
       "      <td>$MU</td>\n",
       "      <td>2018-08-16 21:15:30+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>$MU 5 min chart you can see Where the long ter...</td>\n",
       "      <td>$MU</td>\n",
       "      <td>2018-08-09 14:09:52+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment_score                                    stocktwit_tweet ticker  \\\n",
       "0                 3  $AMD going up but hesitating however chart is ...   $AMD   \n",
       "1                 3  @inforlong @MariaGascon Despite\\nChina trade w...   $CAT   \n",
       "2                 2                                         $AVGO WTF?  $AVGO   \n",
       "3                 2  $PH\\n New Insider Filing On: \\n MULLER KLAUS P...    $PH   \n",
       "4                 3  $FB if it bounces tommorrow do the right thing...    $FB   \n",
       "5                 0                $FB as usual......RED  \\nevery day.    $FB   \n",
       "6                 2  $AET UnitedHealth Group Goes Ex-Dividend Today...   $AET   \n",
       "7                 4             $NOC let&#39;s see when we can hit 310   $NOC   \n",
       "8                 2  @InCyD3R @Ferrabi I bet you‚Äôre also the one sa...  $AMZN   \n",
       "9                 2                             $AMD Pastor the Stooge   $AMD   \n",
       "10                2  $NFLX called it Friday hahahaha suckers sold F...  $NFLX   \n",
       "11                2  $gww EXP:7/20/2018|MaxPain:292.5|HighPutOI:250...   $gww   \n",
       "12                2  $NFLX Day in, Day out this puppy is making peo...  $NFLX   \n",
       "13                2                                              $AAPL  $AAPL   \n",
       "14                2                                              $ANTM  $ANTM   \n",
       "15                2  $NWL Earnings August 06 BMO. 17-Aug-18 Straddl...   $NWL   \n",
       "16                4                                   $AMD buying more   $AMD   \n",
       "17                0  $mu mms took out the SLs, yet it still won&#39...    $mu   \n",
       "18                2  $MU it‚Äôs not even a downtrend - it‚Äôs a flush. ...    $MU   \n",
       "19                3  $MU 5 min chart you can see Where the long ter...    $MU   \n",
       "\n",
       "                    timestamp  \n",
       "0   2018-09-19 18:38:28+00:00  \n",
       "1   2018-10-09 03:51:06+00:00  \n",
       "2   2018-07-12 13:35:32+00:00  \n",
       "3   2018-07-19 03:32:50+00:00  \n",
       "4   2018-08-23 19:07:54+00:00  \n",
       "5   2018-08-02 11:40:49+00:00  \n",
       "6   2018-09-07 15:01:06+00:00  \n",
       "7   2018-09-12 20:48:40+00:00  \n",
       "8   2018-10-28 01:19:27+00:00  \n",
       "9   2018-08-17 13:35:00+00:00  \n",
       "10  2018-08-27 12:51:21+00:00  \n",
       "11  2018-07-17 14:05:07+00:00  \n",
       "12  2018-10-18 15:36:57+00:00  \n",
       "13  2018-10-06 21:38:04+00:00  \n",
       "14  2018-08-08 13:34:42+00:00  \n",
       "15  2018-08-03 13:50:59+00:00  \n",
       "16  2018-09-18 15:45:22+00:00  \n",
       "17  2018-09-05 14:23:16+00:00  \n",
       "18  2018-08-16 21:15:30+00:00  \n",
       "19  2018-08-09 14:09:52+00:00  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json = pd.DataFrame(json_data)\n",
    "df_json.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOHmcU8R6OE8"
   },
   "source": [
    "#Text Preprocessing and Data cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpDCAuo76T8p"
   },
   "source": [
    "##Converting tweets to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "vKuq7j_fSGlS",
    "outputId": "0ba134bb-8f01-434f-beee-b9551d6334b9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>stocktwit_tweet</th>\n",
       "      <th>ticker</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>$AMD going up but hesitating however chart is ...</td>\n",
       "      <td>$AMD</td>\n",
       "      <td>2018-09-19 18:38:28+00:00</td>\n",
       "      <td>$amd going up but hesitating however chart is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>@inforlong @MariaGascon Despite\\nChina trade w...</td>\n",
       "      <td>$CAT</td>\n",
       "      <td>2018-10-09 03:51:06+00:00</td>\n",
       "      <td>@inforlong @mariagascon despite\\nchina trade w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>$AVGO WTF?</td>\n",
       "      <td>$AVGO</td>\n",
       "      <td>2018-07-12 13:35:32+00:00</td>\n",
       "      <td>$avgo wtf?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>$PH\\n New Insider Filing On: \\n MULLER KLAUS P...</td>\n",
       "      <td>$PH</td>\n",
       "      <td>2018-07-19 03:32:50+00:00</td>\n",
       "      <td>$ph\\n new insider filing on: \\n muller klaus p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>$FB if it bounces tommorrow do the right thing...</td>\n",
       "      <td>$FB</td>\n",
       "      <td>2018-08-23 19:07:54+00:00</td>\n",
       "      <td>$fb if it bounces tommorrow do the right thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_score                                    stocktwit_tweet ticker  \\\n",
       "0                3  $AMD going up but hesitating however chart is ...   $AMD   \n",
       "1                3  @inforlong @MariaGascon Despite\\nChina trade w...   $CAT   \n",
       "2                2                                         $AVGO WTF?  $AVGO   \n",
       "3                2  $PH\\n New Insider Filing On: \\n MULLER KLAUS P...    $PH   \n",
       "4                3  $FB if it bounces tommorrow do the right thing...    $FB   \n",
       "\n",
       "                   timestamp  \\\n",
       "0  2018-09-19 18:38:28+00:00   \n",
       "1  2018-10-09 03:51:06+00:00   \n",
       "2  2018-07-12 13:35:32+00:00   \n",
       "3  2018-07-19 03:32:50+00:00   \n",
       "4  2018-08-23 19:07:54+00:00   \n",
       "\n",
       "                                          text_clean  \n",
       "0  $amd going up but hesitating however chart is ...  \n",
       "1  @inforlong @mariagascon despite\\nchina trade w...  \n",
       "2                                         $avgo wtf?  \n",
       "3  $ph\\n new insider filing on: \\n muller klaus p...  \n",
       "4  $fb if it bounces tommorrow do the right thing...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json[\"text_clean\"] = df_json[\"stocktwit_tweet\"].str.lower()\n",
    "df_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ud8Y69VO6bah"
   },
   "source": [
    "##Removing Tickers and Mentions from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VmPufDYSGlX"
   },
   "outputs": [],
   "source": [
    "df_json[\"text_clean\"] = df_json[\"text_clean\"].replace('\\$\\w+',' ',regex=True)\n",
    "df_json[\"text_clean\"] = df_json[\"text_clean\"].replace('\\@\\w+',' ',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "psgTVQAiSGla",
    "outputId": "36079196-9fff-425d-fbeb-abd1dfa7e900"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>stocktwit_tweet</th>\n",
       "      <th>ticker</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>$AMD going up but hesitating however chart is ...</td>\n",
       "      <td>$AMD</td>\n",
       "      <td>2018-09-19 18:38:28+00:00</td>\n",
       "      <td>going up but hesitating however chart is ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>@inforlong @MariaGascon Despite\\nChina trade w...</td>\n",
       "      <td>$CAT</td>\n",
       "      <td>2018-10-09 03:51:06+00:00</td>\n",
       "      <td>despite\\nchina trade war   held very well üëç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>$AVGO WTF?</td>\n",
       "      <td>$AVGO</td>\n",
       "      <td>2018-07-12 13:35:32+00:00</td>\n",
       "      <td>wtf?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>$PH\\n New Insider Filing On: \\n MULLER KLAUS P...</td>\n",
       "      <td>$PH</td>\n",
       "      <td>2018-07-19 03:32:50+00:00</td>\n",
       "      <td>\\n new insider filing on: \\n muller klaus pet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>$FB if it bounces tommorrow do the right thing...</td>\n",
       "      <td>$FB</td>\n",
       "      <td>2018-08-23 19:07:54+00:00</td>\n",
       "      <td>if it bounces tommorrow do the right thing a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>$FB as usual......RED  \\nevery day.</td>\n",
       "      <td>$FB</td>\n",
       "      <td>2018-08-02 11:40:49+00:00</td>\n",
       "      <td>as usual......red  \\nevery day.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>$AET UnitedHealth Group Goes Ex-Dividend Today...</td>\n",
       "      <td>$AET</td>\n",
       "      <td>2018-09-07 15:01:06+00:00</td>\n",
       "      <td>unitedhealth group goes ex-dividend today ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>$NOC let&amp;#39;s see when we can hit 310</td>\n",
       "      <td>$NOC</td>\n",
       "      <td>2018-09-12 20:48:40+00:00</td>\n",
       "      <td>let&amp;#39;s see when we can hit 310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>@InCyD3R @Ferrabi I bet you‚Äôre also the one sa...</td>\n",
       "      <td>$AMZN</td>\n",
       "      <td>2018-10-28 01:19:27+00:00</td>\n",
       "      <td>i bet you‚Äôre also the one saying   should ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>$AMD Pastor the Stooge</td>\n",
       "      <td>$AMD</td>\n",
       "      <td>2018-08-17 13:35:00+00:00</td>\n",
       "      <td>pastor the stooge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>$NFLX called it Friday hahahaha suckers sold F...</td>\n",
       "      <td>$NFLX</td>\n",
       "      <td>2018-08-27 12:51:21+00:00</td>\n",
       "      <td>called it friday hahahaha suckers sold friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>$gww EXP:7/20/2018|MaxPain:292.5|HighPutOI:250...</td>\n",
       "      <td>$gww</td>\n",
       "      <td>2018-07-17 14:05:07+00:00</td>\n",
       "      <td>exp:7/20/2018|maxpain:292.5|highputoi:250.0(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>$NFLX Day in, Day out this puppy is making peo...</td>\n",
       "      <td>$NFLX</td>\n",
       "      <td>2018-10-18 15:36:57+00:00</td>\n",
       "      <td>day in, day out this puppy is making people ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>$AAPL</td>\n",
       "      <td>$AAPL</td>\n",
       "      <td>2018-10-06 21:38:04+00:00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>$ANTM</td>\n",
       "      <td>$ANTM</td>\n",
       "      <td>2018-08-08 13:34:42+00:00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>$NWL Earnings August 06 BMO. 17-Aug-18 Straddl...</td>\n",
       "      <td>$NWL</td>\n",
       "      <td>2018-08-03 13:50:59+00:00</td>\n",
       "      <td>earnings august 06 bmo. 17-aug-18 straddle i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>$AMD buying more</td>\n",
       "      <td>$AMD</td>\n",
       "      <td>2018-09-18 15:45:22+00:00</td>\n",
       "      <td>buying more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>$mu mms took out the SLs, yet it still won&amp;#39...</td>\n",
       "      <td>$mu</td>\n",
       "      <td>2018-09-05 14:23:16+00:00</td>\n",
       "      <td>mms took out the sls, yet it still won&amp;#39;t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>$MU it‚Äôs not even a downtrend - it‚Äôs a flush. ...</td>\n",
       "      <td>$MU</td>\n",
       "      <td>2018-08-16 21:15:30+00:00</td>\n",
       "      <td>it‚Äôs not even a downtrend - it‚Äôs a flush. al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>$MU 5 min chart you can see Where the long ter...</td>\n",
       "      <td>$MU</td>\n",
       "      <td>2018-08-09 14:09:52+00:00</td>\n",
       "      <td>5 min chart you can see where the long term ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment_score                                    stocktwit_tweet ticker  \\\n",
       "0                 3  $AMD going up but hesitating however chart is ...   $AMD   \n",
       "1                 3  @inforlong @MariaGascon Despite\\nChina trade w...   $CAT   \n",
       "2                 2                                         $AVGO WTF?  $AVGO   \n",
       "3                 2  $PH\\n New Insider Filing On: \\n MULLER KLAUS P...    $PH   \n",
       "4                 3  $FB if it bounces tommorrow do the right thing...    $FB   \n",
       "5                 0                $FB as usual......RED  \\nevery day.    $FB   \n",
       "6                 2  $AET UnitedHealth Group Goes Ex-Dividend Today...   $AET   \n",
       "7                 4             $NOC let&#39;s see when we can hit 310   $NOC   \n",
       "8                 2  @InCyD3R @Ferrabi I bet you‚Äôre also the one sa...  $AMZN   \n",
       "9                 2                             $AMD Pastor the Stooge   $AMD   \n",
       "10                2  $NFLX called it Friday hahahaha suckers sold F...  $NFLX   \n",
       "11                2  $gww EXP:7/20/2018|MaxPain:292.5|HighPutOI:250...   $gww   \n",
       "12                2  $NFLX Day in, Day out this puppy is making peo...  $NFLX   \n",
       "13                2                                              $AAPL  $AAPL   \n",
       "14                2                                              $ANTM  $ANTM   \n",
       "15                2  $NWL Earnings August 06 BMO. 17-Aug-18 Straddl...   $NWL   \n",
       "16                4                                   $AMD buying more   $AMD   \n",
       "17                0  $mu mms took out the SLs, yet it still won&#39...    $mu   \n",
       "18                2  $MU it‚Äôs not even a downtrend - it‚Äôs a flush. ...    $MU   \n",
       "19                3  $MU 5 min chart you can see Where the long ter...    $MU   \n",
       "\n",
       "                    timestamp  \\\n",
       "0   2018-09-19 18:38:28+00:00   \n",
       "1   2018-10-09 03:51:06+00:00   \n",
       "2   2018-07-12 13:35:32+00:00   \n",
       "3   2018-07-19 03:32:50+00:00   \n",
       "4   2018-08-23 19:07:54+00:00   \n",
       "5   2018-08-02 11:40:49+00:00   \n",
       "6   2018-09-07 15:01:06+00:00   \n",
       "7   2018-09-12 20:48:40+00:00   \n",
       "8   2018-10-28 01:19:27+00:00   \n",
       "9   2018-08-17 13:35:00+00:00   \n",
       "10  2018-08-27 12:51:21+00:00   \n",
       "11  2018-07-17 14:05:07+00:00   \n",
       "12  2018-10-18 15:36:57+00:00   \n",
       "13  2018-10-06 21:38:04+00:00   \n",
       "14  2018-08-08 13:34:42+00:00   \n",
       "15  2018-08-03 13:50:59+00:00   \n",
       "16  2018-09-18 15:45:22+00:00   \n",
       "17  2018-09-05 14:23:16+00:00   \n",
       "18  2018-08-16 21:15:30+00:00   \n",
       "19  2018-08-09 14:09:52+00:00   \n",
       "\n",
       "                                           text_clean  \n",
       "0     going up but hesitating however chart is ver...  \n",
       "1         despite\\nchina trade war   held very well üëç  \n",
       "2                                                wtf?  \n",
       "3    \\n new insider filing on: \\n muller klaus pet...  \n",
       "4     if it bounces tommorrow do the right thing a...  \n",
       "5                     as usual......red  \\nevery day.  \n",
       "6     unitedhealth group goes ex-dividend today ht...  \n",
       "7                   let&#39;s see when we can hit 310  \n",
       "8       i bet you‚Äôre also the one saying   should ...  \n",
       "9                                   pastor the stooge  \n",
       "10      called it friday hahahaha suckers sold friday  \n",
       "11    exp:7/20/2018|maxpain:292.5|highputoi:250.0(...  \n",
       "12    day in, day out this puppy is making people ...  \n",
       "13                                                     \n",
       "14                                                     \n",
       "15    earnings august 06 bmo. 17-aug-18 straddle i...  \n",
       "16                                        buying more  \n",
       "17    mms took out the sls, yet it still won&#39;t...  \n",
       "18    it‚Äôs not even a downtrend - it‚Äôs a flush. al...  \n",
       "19    5 min chart you can see where the long term ...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19TPyz1-6kub"
   },
   "source": [
    "##Removing URLs from the Tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ezmrDy9USGle"
   },
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_VQK8AQSGli"
   },
   "outputs": [],
   "source": [
    "df_json['text_clean'] = df_json.apply(lambda row: remove_urls(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "0nQNuq51SGlm",
    "outputId": "ea195210-0e35-4e4d-cf84-ab98bfec0595"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       going up but hesitating however chart is ver...\n",
       "1           despite\\nchina trade war   held very well üëç\n",
       "2                                                  wtf?\n",
       "3      \\n new insider filing on: \\n muller klaus pet...\n",
       "4       if it bounces tommorrow do the right thing a...\n",
       "5                       as usual......red  \\nevery day.\n",
       "6           unitedhealth group goes ex-dividend today  \n",
       "7                     let&#39;s see when we can hit 310\n",
       "8         i bet you‚Äôre also the one saying   should ...\n",
       "9                                     pastor the stooge\n",
       "10        called it friday hahahaha suckers sold friday\n",
       "11      exp:7/20/2018|maxpain:292.5|highputoi:250.0(...\n",
       "12      day in, day out this puppy is making people ...\n",
       "13                                                     \n",
       "14                                                     \n",
       "15      earnings august 06 bmo. 17-aug-18 straddle i...\n",
       "16                                          buying more\n",
       "17      mms took out the sls, yet it still won&#39;t...\n",
       "18      it‚Äôs not even a downtrend - it‚Äôs a flush. al...\n",
       "19      5 min chart you can see where the long term ...\n",
       "Name: text_clean, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json['text_clean'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYox_lzX60l8"
   },
   "source": [
    "##Removing html tags from the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LB1aFSNSGlq"
   },
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5pL2BGFSGlt"
   },
   "outputs": [],
   "source": [
    "df_json['text_clean'] = df_json.apply(lambda row: remove_html(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "bS_JFsgSSGlx",
    "outputId": "032c9029-b933-40ab-f4b6-9fd33023b8e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       going up but hesitating however chart is ver...\n",
       "1           despite\\nchina trade war   held very well üëç\n",
       "2                                                  wtf?\n",
       "3      \\n new insider filing on: \\n muller klaus pet...\n",
       "4       if it bounces tommorrow do the right thing a...\n",
       "5                       as usual......red  \\nevery day.\n",
       "6           unitedhealth group goes ex-dividend today  \n",
       "7                     let&#39;s see when we can hit 310\n",
       "8         i bet you‚Äôre also the one saying   should ...\n",
       "9                                     pastor the stooge\n",
       "10        called it friday hahahaha suckers sold friday\n",
       "11      exp:7/20/2018|maxpain:292.5|highputoi:250.0(...\n",
       "12      day in, day out this puppy is making people ...\n",
       "13                                                     \n",
       "14                                                     \n",
       "15      earnings august 06 bmo. 17-aug-18 straddle i...\n",
       "16                                          buying more\n",
       "17      mms took out the sls, yet it still won&#39;t...\n",
       "18      it‚Äôs not even a downtrend - it‚Äôs a flush. al...\n",
       "19      5 min chart you can see where the long term ...\n",
       "Name: text_clean, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json['text_clean'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wxe3RmaY66cg"
   },
   "source": [
    "### Adding Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9FXQV62OVxi9"
   },
   "outputs": [],
   "source": [
    "EMOTICONS = {\n",
    "    u\":‚Äë\\)\":\"Happy face or smiley\",\n",
    "    u\":\\)\":\"Happy face or smiley\",\n",
    "    u\":-\\]\":\"Happy face or smiley\",\n",
    "    u\":\\]\":\"Happy face or smiley\",\n",
    "    u\":-3\":\"Happy face smiley\",\n",
    "    u\":3\":\"Happy face smiley\",\n",
    "    u\":->\":\"Happy face smiley\",\n",
    "    u\":>\":\"Happy face smiley\",\n",
    "    u\"8-\\)\":\"Happy face smiley\",\n",
    "    u\":o\\)\":\"Happy face smiley\",\n",
    "    u\":-\\}\":\"Happy face smiley\",\n",
    "    u\":\\}\":\"Happy face smiley\",\n",
    "    u\":-\\)\":\"Happy face smiley\",\n",
    "    u\":c\\)\":\"Happy face smiley\",\n",
    "    u\":\\^\\)\":\"Happy face smiley\",\n",
    "    u\"=\\]\":\"Happy face smiley\",\n",
    "    u\"=\\)\":\"Happy face smiley\",\n",
    "    u\":‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"X‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":-\\)\\)\":\"Very happy\",\n",
    "    u\":‚Äë\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":-\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‚Äëc\":\"Frown, sad, andry or pouting\",\n",
    "    u\":c\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‚Äë<\":\"Frown, sad, andry or pouting\",\n",
    "    u\":<\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‚Äë\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n",
    "    u\">:\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\{\":\"Frown, sad, andry or pouting\",\n",
    "    u\":@\":\"Frown, sad, andry or pouting\",\n",
    "    u\">:\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":'‚Äë\\(\":\"Crying\",\n",
    "    u\":'\\(\":\"Crying\",\n",
    "    u\":'‚Äë\\)\":\"Tears of happiness\",\n",
    "    u\":'\\)\":\"Tears of happiness\",\n",
    "    u\"D‚Äë':\":\"Horror\",\n",
    "    u\"D:<\":\"Disgust\",\n",
    "    u\"D:\":\"Sadness\",\n",
    "    u\"D8\":\"Great dismay\",\n",
    "    u\"D;\":\"Great dismay\",\n",
    "    u\"D=\":\"Great dismay\",\n",
    "    u\"DX\":\"Great dismay\",\n",
    "    u\":‚ÄëO\":\"Surprise\",\n",
    "    u\":O\":\"Surprise\",\n",
    "    u\":‚Äëo\":\"Surprise\",\n",
    "    u\":o\":\"Surprise\",\n",
    "    u\":-0\":\"Shock\",\n",
    "    u\"8‚Äë0\":\"Yawn\",\n",
    "    u\">:O\":\"Yawn\",\n",
    "    u\":-\\*\":\"Kiss\",\n",
    "    u\":\\*\":\"Kiss\",\n",
    "    u\":X\":\"Kiss\",\n",
    "    u\";‚Äë\\)\":\"Wink or smirk\",\n",
    "    u\";\\)\":\"Wink or smirk\",\n",
    "    u\"\\*-\\)\":\"Wink or smirk\",\n",
    "    u\"\\*\\)\":\"Wink or smirk\",\n",
    "    u\";‚Äë\\]\":\"Wink or smirk\",\n",
    "    u\";\\]\":\"Wink or smirk\",\n",
    "    u\";\\^\\)\":\"Wink or smirk\",\n",
    "    u\":‚Äë,\":\"Wink or smirk\",\n",
    "    u\";D\":\"Wink or smirk\",\n",
    "    u\":‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"X‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‚Äë√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‚Äë/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":‚Äë\\|\":\"Straight face\",\n",
    "    u\":\\|\":\"Straight face\",\n",
    "    u\":$\":\"Embarrassed or blushing\",\n",
    "    u\":‚Äëx\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‚Äë#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‚Äë&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\"O:‚Äë\\)\":\"Angel, saint or innocent\",\n",
    "    u\"O:\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:‚Äë3\":\"Angel, saint or innocent\",\n",
    "    u\"0:3\":\"Angel, saint or innocent\",\n",
    "    u\"0:‚Äë\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:\\)\":\"Angel, saint or innocent\",\n",
    "    u\":‚Äëb\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n",
    "    u\">:‚Äë\\)\":\"Evil or devilish\",\n",
    "    u\">:\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:‚Äë\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:\\)\":\"Evil or devilish\",\n",
    "    u\"3:‚Äë\\)\":\"Evil or devilish\",\n",
    "    u\"3:\\)\":\"Evil or devilish\",\n",
    "    u\">;\\)\":\"Evil or devilish\",\n",
    "    u\"\\|;‚Äë\\)\":\"Cool\",\n",
    "    u\"\\|‚ÄëO\":\"Bored\",\n",
    "    u\":‚ÄëJ\":\"Tongue-in-cheek\",\n",
    "    u\"#‚Äë\\)\":\"Party all night\",\n",
    "    u\"%‚Äë\\)\":\"Drunk or confused\",\n",
    "    u\"%\\)\":\"Drunk or confused\",\n",
    "    u\":-###..\":\"Being sick\",\n",
    "    u\":###..\":\"Being sick\",\n",
    "    u\"<:‚Äë\\|\":\"Dump\",\n",
    "    u\"\\(>_<\\)\":\"Troubled\",\n",
    "    u\"\\(>_<\\)>\":\"Troubled\",\n",
    "    u\"\\(';'\\)\":\"Baby\",\n",
    "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(~_~;\\) \\(„Éª\\.„Éª;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
    "    u\"\\(\\^_-\\)\":\"Wink\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
    "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
    "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
    "    u\"\\^_\\^\":\"Joyful\",\n",
    "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
    "    u\"\\(\\^O\\^\\)Ôºè\":\"Joyful\",\n",
    "    u\"\\(\\^o\\^\\)Ôºè\":\"Joyful\",\n",
    "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"\\('_'\\)\":\"Sad or Crying\",\n",
    "    u\"\\(/_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;_;\":\"Sad of Crying\",\n",
    "    u\"\\(;_:\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;O;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(:_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(ToT\\)\":\"Sad or Crying\",\n",
    "    u\";_;\":\"Sad or Crying\",\n",
    "    u\";-;\":\"Sad or Crying\",\n",
    "    u\";n;\":\"Sad or Crying\",\n",
    "    u\";;\":\"Sad or Crying\",\n",
    "    u\"Q\\.Q\":\"Sad or Crying\",\n",
    "    u\"T\\.T\":\"Sad or Crying\",\n",
    "    u\"QQ\":\"Sad or Crying\",\n",
    "    u\"Q_Q\":\"Sad or Crying\",\n",
    "    u\"\\(-\\.-\\)\":\"Shame\",\n",
    "    u\"\\(-_-\\)\":\"Shame\",\n",
    "    u\"\\(‰∏Ä‰∏Ä\\)\":\"Shame\",\n",
    "    u\"\\(Ôºõ‰∏Ä_‰∏Ä\\)\":\"Shame\",\n",
    "    u\"\\(=_=\\)\":\"Tired\",\n",
    "    u\"\\(=\\^\\¬∑\\^=\\)\":\"cat\",\n",
    "    u\"\\(=\\^\\¬∑\\¬∑\\^=\\)\":\"cat\",\n",
    "    u\"=_\\^=\t\":\"cat\",\n",
    "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
    "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
    "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n",
    "    u\"\\(\\„Éª\\„Éª?\":\"Confusion\",\n",
    "    u\"\\(?_?\\)\":\"Confusion\",\n",
    "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
    "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
    "    u\"\\^/\\^\":\"Normal Laugh\",\n",
    "    u\"\\Ôºà\\*\\^_\\^\\*Ôºâ\" :\"Normal Laugh\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^‚Äî\\^\\Ôºâ\":\"Normal Laugh\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
    "    u\"\\Ôºà\\^‚Äî\\^\\Ôºâ\":\"Waving\",\n",
    "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
    "    u\"\\(-_-\\)/~~~ \\($\\¬∑\\¬∑\\)/~~~\":\"Waving\",\n",
    "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
    "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
    "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
    "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
    "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n",
    "    u'\\(-\"-\\)':\"Worried\",\n",
    "    u\"\\(„Éº„Éº;\\)\":\"Worried\",\n",
    "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
    "    u\"\\(\\ÔºæÔΩñ\\Ôºæ\\)\":\"Happy\",\n",
    "    u\"\\(\\ÔºæÔΩï\\Ôºæ\\)\":\"Happy\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
    "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
    "    u\":O o_O\":\"Surprised\",\n",
    "    u\"o_0\":\"Surprised\",\n",
    "    u\"o\\.O\":\"Surpised\",\n",
    "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
    "    u\"oO\":\"Surprised\",\n",
    "    u\"\\(\\*Ôø£mÔø£\\)\":\"Dissatisfied\",\n",
    "    u\"\\(‚ÄòA`\\)\":\"Snubbed or Deflated\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8d9jtcw7FOb"
   },
   "source": [
    "##Converting Emoticons to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxLvWMAqSGl4"
   },
   "outputs": [],
   "source": [
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\" \").split()), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "irJJ1iTKSGl8"
   },
   "outputs": [],
   "source": [
    "df_json['text_clean'] = df_json.apply(lambda row: convert_emoticons(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "bxAtopzLSGmA",
    "outputId": "c4bba5cd-f0db-454c-fdfb-957f6dc5f5de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       going up but hesitating however chart is ver...\n",
       "1           despite\\nchina trade war   held very well üëç\n",
       "2                                                  wtf?\n",
       "3      \\n new insider filing on: \\n muller klaus pet...\n",
       "4       if it bounces tommorrow do the right thing a...\n",
       "5                       as usual......red  \\nevery day.\n",
       "6           unitedhealth group goes ex-dividend today  \n",
       "7                     let&#39;s see when we can hit 310\n",
       "8         i bet you‚Äôre also the one saying   should ...\n",
       "9                                     pastor the stooge\n",
       "10        called it friday hahahaha suckers sold friday\n",
       "11      exp:7/20/2018|maxpain:292.5|highputoi:250.0(...\n",
       "12      day in, day out this puppy is making people ...\n",
       "13                                                     \n",
       "14                                                     \n",
       "15      earnings august 06 bmo. 17-aug-18 straddle i...\n",
       "16                                          buying more\n",
       "17      mms took out the sls, yet it still won&#39;t...\n",
       "18      it‚Äôs not even a downtrend - it‚Äôs a flush. al...\n",
       "19      5 min chart you can see where the long term ...\n",
       "Name: text_clean, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json['text_clean'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OeLvdyHu7Jsr"
   },
   "source": [
    "##Converting Emojis to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T5-v-fwQSGmF"
   },
   "outputs": [],
   "source": [
    "def convert_emojis(text):\n",
    "    text = emoji.demojize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Y-3IQqBSGmJ"
   },
   "outputs": [],
   "source": [
    "df_json['text_clean'] = df_json.apply(lambda row: convert_emojis(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "MfP2qTeiSGmN",
    "outputId": "682a101e-b3ee-4510-fc74-a4e7df0a144e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       going up but hesitating however chart is ver...\n",
       "1         despite\\nchina trade war   held very well ...\n",
       "2                                                  wtf?\n",
       "3      \\n new insider filing on: \\n muller klaus pet...\n",
       "4       if it bounces tommorrow do the right thing a...\n",
       "5                       as usual......red  \\nevery day.\n",
       "6           unitedhealth group goes ex-dividend today  \n",
       "7                     let&#39;s see when we can hit 310\n",
       "8         i bet you‚Äôre also the one saying   should ...\n",
       "9                                     pastor the stooge\n",
       "10        called it friday hahahaha suckers sold friday\n",
       "11      exp:7/20/2018|maxpain:292.5|highputoi:250.0(...\n",
       "12      day in, day out this puppy is making people ...\n",
       "13                                                     \n",
       "14                                                     \n",
       "15      earnings august 06 bmo. 17-aug-18 straddle i...\n",
       "16                                          buying more\n",
       "17      mms took out the sls, yet it still won&#39;t...\n",
       "18      it‚Äôs not even a downtrend - it‚Äôs a flush. al...\n",
       "19      5 min chart you can see where the long term ...\n",
       "Name: text_clean, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json['text_clean'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nvKWee1sSGmR",
    "outputId": "9bccc8b7-e5a1-44b5-ded7-576a23522513"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(df_json['text_clean'][12]).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.525"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(df_json['text_clean'][12]).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'day in day out this puppy is making people rich face with tears of joy man shrugging light skin tone'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json['text_clean'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UiR1O0Li7OpX"
   },
   "source": [
    "##Removing Punctutations from Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4G9P22ioSGmX"
   },
   "outputs": [],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans(PUNCT_TO_REMOVE, ' ' * len(string.punctuation))).replace(' '*4, ' ').replace(' '*3, ' ').replace(' '*2, ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWiZP7yESGmc"
   },
   "outputs": [],
   "source": [
    "df_json['text_clean'] = df_json.apply(lambda row: remove_punctuation(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "f0EL5mvWSGmi",
    "outputId": "aead6c17-262a-4b67-f353-3bc4571feb7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     going up but hesitating however chart is very ...\n",
       "1     despite\\nchina trade war held very well thumbs up\n",
       "2                                                   wtf\n",
       "3     new insider filing on \\n muller klaus peter\\nt...\n",
       "4     if it bounces tommorrow do the right thing and...\n",
       "5                              as usual red \\nevery day\n",
       "6             unitedhealth group goes ex dividend today\n",
       "7                      let 39 s see when we can hit 310\n",
       "8        i bet you‚Äôre also the one saying should be smh\n",
       "9                                     pastor the stooge\n",
       "10        called it friday hahahaha suckers sold friday\n",
       "11    exp 7 20 2018 maxpain 292 5 highputoi 250 0 78...\n",
       "12    day in day out this puppy is making people ric...\n",
       "13                                                     \n",
       "14                                                     \n",
       "15    earnings august 06 bmo 17 aug 18 straddle impl...\n",
       "16                                          buying more\n",
       "17    mms took out the sls yet it still won 39 t bou...\n",
       "18    it‚Äôs not even a downtrend it‚Äôs a flush almost ...\n",
       "19    5 min chart you can see where the long term su...\n",
       "Name: text_clean, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json['text_clean'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4D6MqIp7U1S"
   },
   "source": [
    "##Stripping whitespaces from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2O7MH-OSGml"
   },
   "outputs": [],
   "source": [
    "def line_strip(text):\n",
    "    text = text.replace('\\n',' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Anh_534MSGmo"
   },
   "outputs": [],
   "source": [
    "df_json['text_clean'] = df_json.apply(lambda row: line_strip(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "y4DFxFb_SGms",
    "outputId": "9fd51368-2f05-4094-bf1c-598f85f67197"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     going up but hesitating however chart is very ...\n",
       "1      despite china trade war held very well thumbs up\n",
       "2                                                   wtf\n",
       "3     new insider filing on   muller klaus peter tra...\n",
       "4     if it bounces tommorrow do the right thing and...\n",
       "5                               as usual red  every day\n",
       "6             unitedhealth group goes ex dividend today\n",
       "7                      let 39 s see when we can hit 310\n",
       "8        i bet you‚Äôre also the one saying should be smh\n",
       "9                                     pastor the stooge\n",
       "10        called it friday hahahaha suckers sold friday\n",
       "11    exp 7 20 2018 maxpain 292 5 highputoi 250 0 78...\n",
       "12    day in day out this puppy is making people ric...\n",
       "13                                                     \n",
       "14                                                     \n",
       "15    earnings august 06 bmo 17 aug 18 straddle impl...\n",
       "16                                          buying more\n",
       "17    mms took out the sls yet it still won 39 t bou...\n",
       "18    it‚Äôs not even a downtrend it‚Äôs a flush almost ...\n",
       "19    5 min chart you can see where the long term su...\n",
       "Name: text_clean, dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json['text_clean'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pFIlUNy17hzm"
   },
   "source": [
    "##Removing Numbers from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8oW7eI_SGmx"
   },
   "outputs": [],
   "source": [
    "def removeNumbers(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FliX7_XgSGmz"
   },
   "outputs": [],
   "source": [
    "df_json['text_clean'] = df_json.apply(lambda row: removeNumbers(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "JcF7L1O3SGm4",
    "outputId": "566fac27-7c71-4cbd-a451-a70a8f2c6f19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     going up but hesitating however chart is very ...\n",
       "1      despite china trade war held very well thumbs up\n",
       "2                                                   wtf\n",
       "3     new insider filing on   muller klaus peter tra...\n",
       "4     if it bounces tommorrow do the right thing and...\n",
       "5                               as usual red  every day\n",
       "6             unitedhealth group goes ex dividend today\n",
       "7                           let  s see when we can hit \n",
       "8        i bet you‚Äôre also the one saying should be smh\n",
       "9                                     pastor the stooge\n",
       "10        called it friday hahahaha suckers sold friday\n",
       "11    exp    maxpain   highputoi   Confusion highcal...\n",
       "12    day in day out this puppy is making people ric...\n",
       "13                                                     \n",
       "14                                                     \n",
       "Name: text_clean, dtype: object"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json['text_clean'].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cd3_k4-_7okw"
   },
   "source": [
    "##Removing Empty rows from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AG3RBduXbB4R",
    "outputId": "395e0c83-be63-4cd1-a219-b0d3b9c11a4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1039131"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json['text_clean'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEx4ZCFcSGm7"
   },
   "outputs": [],
   "source": [
    "df_json = df_json[df_json['text_clean']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QRb5XRalSGm-",
    "outputId": "49cd6da5-65eb-4b92-ab13-d506d584c8b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1011696"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json['text_clean'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "appos = {\n",
    "\"aren‚Äôt\" : \"are not\",\n",
    "\"can‚Äôt\" : \"cannot\",\n",
    "\"couldn‚Äôt\" : \"could not\",\n",
    "\"didn‚Äôt\" : \"did not\",\n",
    "\"doesn‚Äôt\" : \"does not\",\n",
    "\"don‚Äôt\" : \"do not\",\n",
    "\"hadn‚Äôt\" : \"had not\",\n",
    "\"hasn‚Äôt\" : \"has not\",\n",
    "\"haven‚Äôt\" : \"have not\",\n",
    "\"he‚Äôd\" : \"he would\",\n",
    "\"he‚Äôll\" : \"he will\",\n",
    "\"he‚Äôs\" : \"he is\",\n",
    "\"here‚Äôs\": \"here is\",\n",
    "\"i‚Äôd\" : \"I would\",\n",
    "\"i‚Äôd\" : \"I had\",\n",
    "\"i‚Äôll\" : \"I will\",\n",
    "\"i‚Äôm\" : \"I am\",\n",
    "\"isn‚Äôt\" : \"is not\",\n",
    "\"it‚Äôs\" : \"it is\",\n",
    "\"it‚Äôll\":\"it will\",\n",
    "\"i‚Äôve\" : \"I have\",\n",
    "\"let‚Äôs\" : \"let us\",\n",
    "\"mightn‚Äôt\" : \"might not\",\n",
    "\"mustn‚Äôt\" : \"must not\",\n",
    "\"shan‚Äôt\" : \"shall not\",\n",
    "\"she‚Äôd\" : \"she would\",\n",
    "\"she‚Äôll\" : \"she will\",\n",
    "\"she‚Äôs\" : \"she is\",\n",
    "\"shouldn‚Äôt\" : \"should not\",\n",
    "\"that‚Äôs\" : \"that is\",\n",
    "\"there‚Äôs\" : \"there is\",\n",
    "\"they‚Äôd\" : \"they would\",\n",
    "\"they‚Äôll\" : \"they will\",\n",
    "\"they‚Äôre\" : \"they are\",\n",
    "\"they‚Äôve\" : \"they have\",\n",
    "\"we‚Äôd\" : \"we would\",\n",
    "\"we‚Äôre\" : \"we are\",\n",
    "\"weren‚Äôt\" : \"were not\",\n",
    "\"we‚Äôve\" : \"we have\",\n",
    "\"what‚Äôll\" : \"what will\",\n",
    "\"what‚Äôre\" : \"what are\",\n",
    "\"what‚Äôs\" : \"what is\",\n",
    "\"what‚Äôve\" : \"what have\",\n",
    "\"where‚Äôs\" : \"where is\",\n",
    "\"who‚Äôd\" : \"who would\",\n",
    "\"who‚Äôll\" : \"who will\",\n",
    "\"who‚Äôre\" : \"who are\",\n",
    "\"who‚Äôs\" : \"who is\",\n",
    "\"who‚Äôve\" : \"who have\",\n",
    "\"won‚Äôt\" : \"will not\",\n",
    "\"wouldn‚Äôt\" : \"would not\",\n",
    "\"y‚Äôall\":\"you all\",\n",
    "\"you‚Äôd\" : \"you would\",\n",
    "\"you‚Äôll\" : \"you will\",\n",
    "\"you‚Äôre\" : \"you are\",\n",
    "\"you‚Äôve\" : \"you have\",\n",
    "\"‚Äôre\": \" are\",\n",
    "\"wasn‚Äôt\": \"was not\",\n",
    "\"we‚Äôll\":\" will\",\n",
    "\"didn‚Äôt\": \"did not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apostrophe_lookup(sentence):\n",
    "    return ' '.join([appos[word] if word in appos else word for word in sentence.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord(\"‚Äô\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json['text_clean'] = df_json.apply(lambda row: apostrophe_lookup(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1039110                           what color does alisa like\n",
       "1039111    when analysts cut price targets and stock drop...\n",
       "1039112    estimize revenue expectations are higher than ...\n",
       "1039113    back in the day this would fall on a down day ...\n",
       "1039114    altria group moConfusion announces earnings ep...\n",
       "1039115    new insider filing on ravener robert d transac...\n",
       "1039116                            outperforms of all stocks\n",
       "1039117                                     russia not happy\n",
       "1039118                                              crushed\n",
       "1039119    raise happened recently so not much impact on ...\n",
       "1039120                             this your entrance point\n",
       "1039121    inverted hammer rd time is a charm what do you...\n",
       "1039122    m ago sec current event sConfusion report regu...\n",
       "1039123    do not even play you all know it is gapping up...\n",
       "1039124    i wonder why guy adami is always bearish on th...\n",
       "1039125                prepare yourselves for launch closing\n",
       "1039127                                       fire fire fire\n",
       "1039128       damn should of shorted after hours to late now\n",
       "1039129                                        back to the s\n",
       "1039130                             lagging other tech today\n",
       "Name: text_clean, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json['text_clean'].tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6w9wW3mH7vl7"
   },
   "source": [
    "##Removing Stop Words from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = (stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in ENGLISH_STOP_WORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json['text_clean'] = df_json.apply(lambda row: remove_stopwords(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    return unidecode.unidecode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json['text_clean'] = df_json.apply(lambda row: remove_diacritics(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words_str = \"\"\"\n",
    "AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime Anywhere Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It is Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great\n",
    "G9=Genius\n",
    "GTFO=Get the Fuck Out\n",
    "IC=I See\n",
    "ICQ=I Seek you\n",
    "ILU= I Love You\n",
    "IMHO=In My Honest Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My Ass Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The Ass\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My Ass Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "SMH=Shaking My Head\n",
    "ASL=Age Sex Location\n",
    "THX=Thank You\n",
    "TTFN=Ta Ta For Now \n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The Fuck\n",
    "WTG=Way To Go\n",
    "WUF=Where Are You From\n",
    "W8=Wait\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words_map_dict = {}\n",
    "chat_words_list = []\n",
    "for line in chat_words_str.split(\"\\n\"):\n",
    "    if line != \"\":\n",
    "        cw = line.split(\"=\")[0]\n",
    "        cw_expanded = line.split(\"=\")[1]\n",
    "        chat_words_list.append(cw)\n",
    "        chat_words_map_dict[cw] = cw_expanded\n",
    "chat_words_list = set(chat_words_list)\n",
    "\n",
    "def chat_words_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words_list:\n",
    "            new_text.append(chat_words_map_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json['text_clean'] = df_json.apply(lambda row: chat_words_conversion(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>stocktwit_tweet</th>\n",
       "      <th>ticker</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>$AMD going up but hesitating however chart is ...</td>\n",
       "      <td>$AMD</td>\n",
       "      <td>2018-09-19 18:38:28+00:00</td>\n",
       "      <td>going hesitating chart stable going upward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>@inforlong @MariaGascon Despite\\nChina trade w...</td>\n",
       "      <td>$CAT</td>\n",
       "      <td>2018-10-09 03:51:06+00:00</td>\n",
       "      <td>despite china trade war held thumbs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>$AVGO WTF?</td>\n",
       "      <td>$AVGO</td>\n",
       "      <td>2018-07-12 13:35:32+00:00</td>\n",
       "      <td>what the fuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>$PH\\n New Insider Filing On: \\n MULLER KLAUS P...</td>\n",
       "      <td>$PH</td>\n",
       "      <td>2018-07-19 03:32:50+00:00</td>\n",
       "      <td>new insider filing muller klaus peter transact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>$FB if it bounces tommorrow do the right thing...</td>\n",
       "      <td>$FB</td>\n",
       "      <td>2018-08-23 19:07:54+00:00</td>\n",
       "      <td>bounces tommorrow right thing get the fuck out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>$FB as usual......RED  \\nevery day.</td>\n",
       "      <td>$FB</td>\n",
       "      <td>2018-08-02 11:40:49+00:00</td>\n",
       "      <td>usual red day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>$AET UnitedHealth Group Goes Ex-Dividend Today...</td>\n",
       "      <td>$AET</td>\n",
       "      <td>2018-09-07 15:01:06+00:00</td>\n",
       "      <td>unitedhealth group goes ex dividend today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>$NOC let&amp;#39;s see when we can hit 310</td>\n",
       "      <td>$NOC</td>\n",
       "      <td>2018-09-12 20:48:40+00:00</td>\n",
       "      <td>let s hit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>@InCyD3R @Ferrabi I bet you‚Äôre also the one sa...</td>\n",
       "      <td>$AMZN</td>\n",
       "      <td>2018-10-28 01:19:27+00:00</td>\n",
       "      <td>bet saying shaking my head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>$AMD Pastor the Stooge</td>\n",
       "      <td>$AMD</td>\n",
       "      <td>2018-08-17 13:35:00+00:00</td>\n",
       "      <td>pastor stooge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>$NFLX called it Friday hahahaha suckers sold F...</td>\n",
       "      <td>$NFLX</td>\n",
       "      <td>2018-08-27 12:51:21+00:00</td>\n",
       "      <td>called friday hahahaha suckers sold friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>$gww EXP:7/20/2018|MaxPain:292.5|HighPutOI:250...</td>\n",
       "      <td>$gww</td>\n",
       "      <td>2018-07-17 14:05:07+00:00</td>\n",
       "      <td>exp maxpain highputoi confusion highcalloihapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>$NFLX Day in, Day out this puppy is making peo...</td>\n",
       "      <td>$NFLX</td>\n",
       "      <td>2018-10-18 15:36:57+00:00</td>\n",
       "      <td>day day puppy making people rich face tears jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>$NWL Earnings August 06 BMO. 17-Aug-18 Straddl...</td>\n",
       "      <td>$NWL</td>\n",
       "      <td>2018-08-03 13:50:59+00:00</td>\n",
       "      <td>earnings august bmo aug straddle implies +- vs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>$AMD buying more</td>\n",
       "      <td>$AMD</td>\n",
       "      <td>2018-09-18 15:45:22+00:00</td>\n",
       "      <td>buying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>$mu mms took out the SLs, yet it still won&amp;#39...</td>\n",
       "      <td>$mu</td>\n",
       "      <td>2018-09-05 14:23:16+00:00</td>\n",
       "      <td>mms took sls won t bounce pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>$MU it‚Äôs not even a downtrend - it‚Äôs a flush. ...</td>\n",
       "      <td>$MU</td>\n",
       "      <td>2018-08-16 21:15:30+00:00</td>\n",
       "      <td>downtrend flush feels deliberate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>$MU 5 min chart you can see Where the long ter...</td>\n",
       "      <td>$MU</td>\n",
       "      <td>2018-08-09 14:09:52+00:00</td>\n",
       "      <td>min chart long term support trend line downtre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>$MU you all know that when it drops in one day...</td>\n",
       "      <td>$MU</td>\n",
       "      <td>2018-09-06 19:32:52+00:00</td>\n",
       "      <td>know drops day really comes know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>Here‚Äôs what 6 Estimize analysts believe $EIX w...</td>\n",
       "      <td>$EIX</td>\n",
       "      <td>2018-07-25 12:39:38+00:00</td>\n",
       "      <td>estimize analysts believe report q revenue rep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment_score                                    stocktwit_tweet ticker  \\\n",
       "0                 3  $AMD going up but hesitating however chart is ...   $AMD   \n",
       "1                 3  @inforlong @MariaGascon Despite\\nChina trade w...   $CAT   \n",
       "2                 2                                         $AVGO WTF?  $AVGO   \n",
       "3                 2  $PH\\n New Insider Filing On: \\n MULLER KLAUS P...    $PH   \n",
       "4                 3  $FB if it bounces tommorrow do the right thing...    $FB   \n",
       "5                 0                $FB as usual......RED  \\nevery day.    $FB   \n",
       "6                 2  $AET UnitedHealth Group Goes Ex-Dividend Today...   $AET   \n",
       "7                 4             $NOC let&#39;s see when we can hit 310   $NOC   \n",
       "8                 2  @InCyD3R @Ferrabi I bet you‚Äôre also the one sa...  $AMZN   \n",
       "9                 2                             $AMD Pastor the Stooge   $AMD   \n",
       "10                2  $NFLX called it Friday hahahaha suckers sold F...  $NFLX   \n",
       "11                2  $gww EXP:7/20/2018|MaxPain:292.5|HighPutOI:250...   $gww   \n",
       "12                2  $NFLX Day in, Day out this puppy is making peo...  $NFLX   \n",
       "15                2  $NWL Earnings August 06 BMO. 17-Aug-18 Straddl...   $NWL   \n",
       "16                4                                   $AMD buying more   $AMD   \n",
       "17                0  $mu mms took out the SLs, yet it still won&#39...    $mu   \n",
       "18                2  $MU it‚Äôs not even a downtrend - it‚Äôs a flush. ...    $MU   \n",
       "19                3  $MU 5 min chart you can see Where the long ter...    $MU   \n",
       "20                1  $MU you all know that when it drops in one day...    $MU   \n",
       "21                2  Here‚Äôs what 6 Estimize analysts believe $EIX w...   $EIX   \n",
       "\n",
       "                    timestamp  \\\n",
       "0   2018-09-19 18:38:28+00:00   \n",
       "1   2018-10-09 03:51:06+00:00   \n",
       "2   2018-07-12 13:35:32+00:00   \n",
       "3   2018-07-19 03:32:50+00:00   \n",
       "4   2018-08-23 19:07:54+00:00   \n",
       "5   2018-08-02 11:40:49+00:00   \n",
       "6   2018-09-07 15:01:06+00:00   \n",
       "7   2018-09-12 20:48:40+00:00   \n",
       "8   2018-10-28 01:19:27+00:00   \n",
       "9   2018-08-17 13:35:00+00:00   \n",
       "10  2018-08-27 12:51:21+00:00   \n",
       "11  2018-07-17 14:05:07+00:00   \n",
       "12  2018-10-18 15:36:57+00:00   \n",
       "15  2018-08-03 13:50:59+00:00   \n",
       "16  2018-09-18 15:45:22+00:00   \n",
       "17  2018-09-05 14:23:16+00:00   \n",
       "18  2018-08-16 21:15:30+00:00   \n",
       "19  2018-08-09 14:09:52+00:00   \n",
       "20  2018-09-06 19:32:52+00:00   \n",
       "21  2018-07-25 12:39:38+00:00   \n",
       "\n",
       "                                           text_clean  \n",
       "0          going hesitating chart stable going upward  \n",
       "1                 despite china trade war held thumbs  \n",
       "2                                       what the fuck  \n",
       "3   new insider filing muller klaus peter transact...  \n",
       "4      bounces tommorrow right thing get the fuck out  \n",
       "5                                       usual red day  \n",
       "6           unitedhealth group goes ex dividend today  \n",
       "7                                           let s hit  \n",
       "8                          bet saying shaking my head  \n",
       "9                                       pastor stooge  \n",
       "10         called friday hahahaha suckers sold friday  \n",
       "11  exp maxpain highputoi confusion highcalloihapp...  \n",
       "12  day day puppy making people rich face tears jo...  \n",
       "15  earnings august bmo aug straddle implies +- vs...  \n",
       "16                                             buying  \n",
       "17                      mms took sls won t bounce pos  \n",
       "18                   downtrend flush feels deliberate  \n",
       "19  min chart long term support trend line downtre...  \n",
       "20                   know drops day really comes know  \n",
       "21  estimize analysts believe report q revenue rep...  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('face', 55592),\n",
       " ('confusion', 54037),\n",
       " ('short', 49587),\n",
       " ('buy', 48624),\n",
       " ('today', 48467),\n",
       " ('just', 48411),\n",
       " ('stock', 37052),\n",
       " ('earnings', 35784),\n",
       " ('calls', 35279),\n",
       " ('estimize', 34077)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df_json[\"text_clean\"].values:\n",
    "    for word in text.split():\n",
    "        if len(word)>1:\n",
    "            cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"btd\"',\n",
       " 'aaaamazzzing',\n",
       " 'artard',\n",
       " 'bouncingback',\n",
       " 'brzl',\n",
       " 'chuggs',\n",
       " 'dikshit',\n",
       " 'econoline',\n",
       " 'fastrack',\n",
       " 'goodrx',\n",
       " 'igent',\n",
       " 'kack',\n",
       " 'kapertrader',\n",
       " 'kiu',\n",
       " 'mwhahahaha',\n",
       " 'nbas',\n",
       " 'needmemory',\n",
       " 'ouija',\n",
       " 'pecans',\n",
       " 'psychical',\n",
       " 'pullish',\n",
       " 'retrade',\n",
       " 'revange',\n",
       " 'shippppppppppp',\n",
       " 'spxu',\n",
       " 'strugling',\n",
       " 'transporting',\n",
       " 'trumpintergestion',\n",
       " 'uinta',\n",
       " 'vox'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rare_words = 30\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "RAREWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RcNJAmcH73GB"
   },
   "source": [
    "#Finding frequently occuring words in Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ph1UkCs2SGnP"
   },
   "outputs": [],
   "source": [
    "def collateText(colName):\n",
    "    text = \" \".join(tweet for tweet in df_json[colName])\n",
    "    print (\"There are {} words in the combination of all review.\".format(len(text)))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hzMDWBRXSGnT",
    "outputId": "96d61058-6d0f-49f5-ccea-84eaa1f64fa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 42192393 words in the combination of all review.\n",
      "6579276\n"
     ]
    }
   ],
   "source": [
    "allText = collateText('text_clean')\n",
    "tokenized_text=word_tokenize(allText)\n",
    "print(len(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dz-SPSk3SGnc"
   },
   "outputs": [],
   "source": [
    "tokenized_text = [token for token in tokenized_text if len(token)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6313988\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mO-cm-V0SGnj",
    "outputId": "09070a96-0452-4f01-8fe0-767d0c69149c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 89101 samples and 6313988 outcomes>\n"
     ]
    }
   ],
   "source": [
    "fdist = FreqDist(tokenized_text)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s7EANjk-91a5"
   },
   "source": [
    "### Top 30 frequently occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "GHKCuhroSGnp",
    "outputId": "e57d308c-fa20-43a6-ca60-70403ab03233"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('face', 55595),\n",
       " ('confusion', 54043),\n",
       " ('short', 49615),\n",
       " ('today', 49040),\n",
       " ('buy', 48726),\n",
       " ('just', 48435),\n",
       " ('stock', 37083),\n",
       " ('earnings', 35793),\n",
       " ('calls', 35288),\n",
       " ('estimize', 34077),\n",
       " ('going', 33443),\n",
       " ('like', 33364),\n",
       " ('day', 31843),\n",
       " ('reporting', 29766),\n",
       " ('good', 28872),\n",
       " ('market', 28779),\n",
       " ('shares', 27207),\n",
       " ('analysts', 27003),\n",
       " ('new', 26657),\n",
       " ('quot', 26035),\n",
       " ('laughing', 25689),\n",
       " ('money', 25504),\n",
       " ('tomorrow', 25343),\n",
       " ('trade', 25093),\n",
       " ('eps', 25076),\n",
       " ('report', 24867),\n",
       " ('volume', 24675),\n",
       " ('week', 24534),\n",
       " ('time', 24212),\n",
       " ('revenue', 22274)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "qFyCp_7JSGnt",
    "outputId": "a3b3b3e1-afd7-4c2e-9d24-de9da6234166"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [14, 8]\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A4F973xQ_Bgn"
   },
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bhn6kErQ_Pye"
   },
   "source": [
    "### Top 5 companies with most tweets with sentiment score as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZYqzxLlGSGnz"
   },
   "outputs": [],
   "source": [
    "df = df_json[df_json['sentiment_score']== 0]['ticker'].value_counts().head(5).to_frame()\n",
    "df.reset_index(level=0,inplace=True)\n",
    "df.columns=['Ticker','Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "plniZmMRSGn6",
    "outputId": "1215ef4e-dab0-48be-f5c4-9cfe2ea9d39a"
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"Ticker\", y=\"Count\", data=df)\n",
    "ax.set_title(\"Top 5 tickers with negative tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7XSV2HE_eKq"
   },
   "source": [
    "### Top 5 companies with most tweets with sentiment score as 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "4CEq3x0HzNd4",
    "outputId": "b0137246-c6ea-4a22-badd-c145a336864f"
   },
   "outputs": [],
   "source": [
    "df = df_json[df_json['sentiment_score']== 4]['ticker'].value_counts().head(5).to_frame()\n",
    "df.reset_index(level=0,inplace=True)\n",
    "df.columns=['Ticker','Count']\n",
    "ax = sns.barplot(x=\"Ticker\", y=\"Count\", data=df)\n",
    "ax.set_title(\"Top 5 tickers with Positive tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06vSxT1kVBTY"
   },
   "source": [
    "## Plot of number of tweets sorted by sentiment scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "colab_type": "code",
    "id": "4JIPeatz_jkx",
    "outputId": "cea2cef1-fa30-42c1-cd91-cec3b35695ff"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=\"sentiment_score\",  data=df_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "77LZXwseXDCV"
   },
   "source": [
    "## Word Cloud for Frequently occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "colab_type": "code",
    "id": "nBQ34mFNA_CW",
    "outputId": "d994abf3-da18-45d0-81fc-caac3a0c2f82"
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"White\", width=3000,height=2000).generate_from_frequencies(fdist)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_-Voz3NSG74"
   },
   "outputs": [],
   "source": [
    "def collateScoreText(colName):\n",
    "    text = \" \".join(tweet for tweet in df_json[df_json['sentiment_score']==2]['text_clean'])\n",
    "    print (\"There are {} words in the combination of all review.\".format(len(text)))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wyvscNgyT7bP"
   },
   "outputs": [],
   "source": [
    "def drawWordCloud(score=0,colName='text_clean'):\n",
    "  token_text=word_tokenize(collateScoreText(colName))\n",
    "  freqdist = FreqDist(token_text)\n",
    "  wordcloud = WordCloud(background_color=\"White\", width=3000,height=2000).generate_from_frequencies(freqdist)\n",
    "  plt.imshow(wordcloud, interpolation='bilinear')\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rEVzAHi1X-wJ"
   },
   "source": [
    "## Word Cloud for most frequent words during very negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "id": "2qfM2vewX8QB",
    "outputId": "e507d090-ad4a-426e-f4c8-ffc8dbbc6467"
   },
   "outputs": [],
   "source": [
    "drawWordCloud(score=0,colName='text_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8nDs3HzYO0H"
   },
   "source": [
    "## Word Cloud for most frequent words during negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "id": "7U843w9AXpYM",
    "outputId": "15ab34ec-0725-4c67-fd4d-18e85259ced3"
   },
   "outputs": [],
   "source": [
    "drawWordCloud(score=1,colName='text_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xK2-DZdTYW6M"
   },
   "source": [
    "## Word Cloud for most frequent words during ok tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "id": "6e-8Qn8-Xaw1",
    "outputId": "91f5035c-0898-453c-c89d-a8a9798d14fa"
   },
   "outputs": [],
   "source": [
    "drawWordCloud(score=2,colName='text_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mmEw5wtXYaqn"
   },
   "source": [
    "## Word Cloud for most frequent words during positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "id": "_GOdBJYAW6CA",
    "outputId": "e8d06717-2a62-43b4-9bfb-0d5ebe07bb89"
   },
   "outputs": [],
   "source": [
    "drawWordCloud(score=3,colName='text_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aL2ktrcnYetb"
   },
   "source": [
    "## Word Cloud for most frequent words during very positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "id": "5r_m5aIBWl4g",
    "outputId": "d21492f7-e07f-4db1-e308-22d7bf7537fe"
   },
   "outputs": [],
   "source": [
    "drawWordCloud(score=4,colName='text_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0K63oeWFZFRh"
   },
   "source": [
    "## Lemmitization of the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_token(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_token(df_json['text_clean'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json['text_tokenize'] = df_json.apply(lambda row: word_token(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TsVbP2h2X4yr"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDJaZUfeYjtf"
   },
   "outputs": [],
   "source": [
    "df_json['text_clean'] = df_json.apply(lambda row: lemmatize_words(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "InK3l8prY8nm",
    "outputId": "273d67a3-19d2-42ab-8bb6-00026508b5fa"
   },
   "outputs": [],
   "source": [
    "df_json['text_clean'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    return TextBlob(text).tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json['pos_tag'] = df_json.apply(lambda row: pos_tagging(row['text_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json['pos_tag'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allText = collateText('text_clean')\n",
    "tokenized_text=word_tokenize(allText)\n",
    "print(len(tokenized_text))\n",
    "tokenized_text = [token for token in tokenized_text if len(token)>1]\n",
    "fdist = FreqDist(tokenized_text)\n",
    "print(fdist)\n",
    "fdist.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "44743155 - 43967701 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json_model = df_json.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=['stocktwit_tweet','ticker','timestamp','text_tokenize','pos_tag']\n",
    "df_json_model = df_json_model.drop(drop_cols,axis=True)\n",
    "df_json_model.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,valX,trainY,valY = train_test_split(df_json.loc[:,df_json.columns!=\"sentiment_score\"],df_json.loc[:,df_json.columns==\"sentiment_score\"],test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX.shape)\n",
    "print(valX.shape)\n",
    "print(trainY.shape)\n",
    "print(valY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS,lowercase=True ,norm=None)\n",
    "trainX_tfidf = tfidf_transformer.fit_transform(trainX['text_clean'])\n",
    "trainX_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(trainX_tfidf,trainY)\n",
    "pred = clf.predict(trainX_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(trainY,pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PhD PreProcess.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
